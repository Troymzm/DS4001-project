# Methodology/方法论

## 任务一：使用VAE完成MNIST图像生成

我们实现了一个基于VAE（变分自编码器）的MNIST图像生成模型。该模型主要包含以下关键组件：

1. **编码器架构**：
   - 使用三层卷积层进行特征提取：32->64->128通道
   - 使用LeakyReLU激活函数，避免梯度消失问题
   - 最终输出潜在空间的均值(μ)和对数方差(log σ²)
2. **解码器架构**：
   - 使用三层转置卷积层进行图像重建：128->64->32->1通道
   - 保持与编码器对称的结构，确保信息能够有效重建
   - 使用Sigmoid激活函数确保输出在[0,1]范围内
3. **潜在空间设计**：
   - 使用20维的潜在空间，在压缩率和重建质量之间取得平衡
4. **损失函数设计**：
   - 重构损失：使用MSE损失衡量重建质量
   - KL散度损失：确保潜在空间分布接近标准正态分布
   - 通过var参数平衡两种损失的权重

# Experiment/实验

## 任务一：使用VAE完成MNIST图像生成

### Setting/设置

我们使用以下超参数进行训练：

```bash
--task VAEwolabel \
--epochs 50 \
--batch_size 1024 \
--lr 5e-3 \
--latent_dim 20 \
--var 0.25
```


### Results/结果

1. **训练结果**：(最后一个epoch)

   - train loss ：38.6

   - validate loss ：39.4

   - loss curve：

     <img src="https://raw.githubusercontent.com/Troymzm/pictureBed/main/20250609130328390.png" alt="loss_curve" style="zoom: 50%;" />

2. **测试结果**：

   - ssim：0.8504231229496682
   - mse：0.013352158863545396
   - grade：10


### Indepth analysis/深入分析

#### 1. 网络架构消融实验

1. **卷积网络的影响**

   其他一致的参数

   ```
   --epochs 50 \
   --batch_size 1024 \
   --lr 5e-3 \
   --latent_dim 20 \
   --var 0.5
   ```

   | model      | ssim                    | mse                  |
   | ---------- | ----------------------- | -------------------- |
   | 仅仅使用全连接层 | 0.7390947758029213 | 0.022465829981012014 |
   | **卷积层** | **0.7853706225521735** |**0.01890210650534719**|

   由实验结果可知，仅使用全连接层的模型在重构质量上明显不如使用卷积层的模型。这是因为卷积层能够更好地捕捉图像中的局部特征，从而提高重构质量。

2. **卷积层数量的影响**：

   其他一致的参数

   ```
   --epochs 50 \
   --batch_size 1024 \
   --lr 5e-3 \
   --latent_dim 20 \
   --var 0.5
   ```

   | 卷积层数量 | ssim                   | mse                     |
   | ---------- | ---------------------- | ----------------------- |
   | 2          | 0.7774402835363021     | 0.019798182930034738    |
   | **3**      | **0.7853706225521735** | **0.01890210650534719** |
   | 4          | 0.034549703277560226   | 0.16805256211308045     |

   由实验结果可知，卷积层数量对模型性能有显著影响。使用2层卷积层的模型性能较好，但3层卷积层的模型提供了最佳的平衡点，能够更好地捕捉图像特征并提高重构质量。而4层卷积层的模型性能急剧下降，这可能是由于过拟合或梯度消失问题导致的。

3. **潜在空间维度的影响**：

   其他一致的参数

   ```
   --epochs 50 \
   --batch_size 1024 \
   --lr 5e-3 \
   --var 0.5
   ```

   | latent_dim | ssim                   | mse                     |
   | ---------- | ---------------------- | ----------------------- |
   | 10         | 0.7823106862653039     | 0.019224410224874245    |
   | **20**     | **0.7853706225521735** | **0.01890210650534719** |
   | 30         | 0.7791554063858023     | 0.0192843736690755      |

   由实验结果可知，潜在空间维度对模型性能影响较小。10和30维的潜在空间虽然性能略低，但仍然能够生成较好的图像。20维的潜在空间提供了最佳的平衡点。

4. **在卷积网络基础上引入注意力机制的影响**

   其他一致的参数

   ```
   --epochs 50 \
   --batch_size 1024 \
   --lr 5e-3 \
   --latent_dim 20 \
   --var 0.5
   ```

   | model                      | ssim                   | mse                     |
   | -------------------------- | ---------------------- | ----------------------- |
   | 卷积层基础上添加注意力机制 | 0.7713305797284312     | 0.020101515988748507    |
   | **卷积层**                 | **0.7853706225521735** | **0.01890210650534719** |

   由实验结果可知，在卷积网络基础上引入注意力机制并未显著提升模型性能，反而略微降低了SSIM和增加了MSE。这可能是因为注意力机制的引入增加了模型的复杂性，而MNIST数据集相对简单，卷积层已经足够捕捉图像特征，因此注意力机制的引入并未带来明显的性能提升。

#### 2. 损失函数分析

1. **var参数的影响**：
   
   其他一致的参数
   
   ```
   --epochs 50 \
   --batch_size 1024 \
   --lr 5e-3 \
   --latent_dim 20 \
   ```
   
   | var  | ssim                   | mse                      |
   | ---- | ---------------------- | ------------------------ |
   | 0.5  | 0.7853706225521735     | 0.01890210650534719      |
   | 0.25 | 0.8504231229496682     | 0.013352158863545396     |
   | 0.22 | **0.8551499862844326** | **0.012753053554755174** |
   | 0.2  | loss 爆炸              | loss 爆炸                |
   
   由实验结果可知，var参数对模型性能有显著影响。较高的var（0.5）导致了较低的SSIM和较高的MSE，这可能是因为KL散度损失过大，导致模型难以学习到有效的潜在空间分布。随着var的降低，模型性能显著提升，0.22的var提供了最佳的平衡点，而0.2的var则导致了loss爆炸，表明KL散度损失过小，模型无法有效学习。但注意到0.25的var虽然性能略低于0.22，但差别不大，为方便因此在最后的模型中使用0.25的var。

#### 3. 训练策略分析

1. **批量大小的影响**：

   其他一致的参数

   ```
   --epochs 50 \
   --lr 5e-3 \
   --latent_dim 20 \
   --var 0.5
   ```

   | batch_size | ssim                   | mse                     |
   | ---------- | ---------------------- | ----------------------- |
   | 512        | 0.2421878643554819     | 0.018998108870999342    |
   | **1024**   | **0.7853706225521735** | **0.01890210650534719** |
   | 2048       | 0.7342257578823198     | 0.024088800722249677    |

   由实验结果可知，小的批量大小（512）导致了较差的SSIM和较高的MSE，这是由于小的批量大小使得模型训练不稳定，容易出现loss爆炸的结果，表明模型在小批量数据上难以学习到有效的特征。随着批量大小的增加，模型的性能显著提升，1024的批量大小提供了最佳的平衡点，而2048的批量大小虽然性能略有下降，但仍然优于512。

2. **学习率的影响**：
   
   其他一致的参数
   
   ```
   --epochs 50 \
   --batch_size 1024 \
   --latent_dim 20 \
   --var 0.5
   ```
   
   | learning_rate | ssim                   | mse                     |
   | ------------- | ---------------------- | ----------------------- |
   | 1e-3          | 0.7823671963318102     | 0.11204825839159166     |
   | **5e-3**      | **0.7853706225521735** | **0.01890210650534719** |
   | 6e-3          | 0.7810330007817275     | 0.019269545453475496    |
   
   由实验结果可知，学习率对模型性能有显著影响。较低的学习率（1e-3）导致了较差的SSIM和较高的MSE，这可能是因为模型收敛速度过慢，未能充分学习数据特征。较高的学习率（6e-3）虽然训练速度更快，但也导致了性能下降，可能是因为模型在训练过程中出现了震荡。5e-3的学习率提供了最佳的平衡点，使得模型能够快速收敛并保持良好的生成质量。
